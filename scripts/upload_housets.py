import pandas as pd
import os
from supabase import create_client, Client
from dotenv import load_dotenv
import numpy as np

def upload_housets():
    # Load env
    load_dotenv()
    if not os.getenv('SUPABASE_URL'):
        load_dotenv(os.path.join(os.path.dirname(os.path.dirname(__file__)), 'backend', '.env'))
        
    url = os.getenv('SUPABASE_URL')
    key = os.getenv('SUPABASE_KEY')
    
    if not url or not key:
        print("Error: Supabase credentials not found.")
        return

    supabase: Client = create_client(url, key)
    
    input_file = 'HouseTS.csv'
    if not os.path.exists(input_file):
        print(f"Error: {input_file} not found.")
        return

    print("Reading HouseTS.csv...")
    # Read only necessary columns to save memory if file is huge
    # Based on grep, columns are: date, median_sale_price, ..., city, zipcode, ...
    df = pd.read_csv(input_file)
    
    # Filter for DC
    print("Filtering for DC...")
    df_dc = df[df['city'] == 'DC'].copy()
    print(f"Found {len(df_dc)} records for DC.")
    
    # Select relevant columns for chatbot
    # Expanded list based on user request
    cols = [
        'date', 'median_sale_price', 'median_list_price', 'median_ppsf', 
        'homes_sold', 'pending_sales', 'new_listings', 'inventory', 
        'median_dom', 'avg_sale_to_list', 'sold_above_list', 'zipcode'
    ]
    df_upload = df_dc[cols].copy()
    
    # Rename columns to match DB (snake_case is already used in CSV, just ensuring consistency)
    # Mapping: CSV -> DB
    df_upload.columns = [
        'date', 'median_sale_price', 'median_list_price', 'median_ppsf', 
        'homes_sold', 'pending_sales', 'new_listings', 'inventory', 
        'median_dom', 'avg_sale_to_list', 'sold_above_list', 'zip_code'
    ]
    
    # Convert types
    df_upload['zip_code'] = df_upload['zip_code'].astype(str)
    df_upload = df_upload.replace({np.nan: None})
    
    records = df_upload.to_dict('records')
    
    print(f"Uploading {len(records)} records to 'house_ts'...")
    
    batch_size = 1000
    total_uploaded = 0
    
    for i in range(0, len(records), batch_size):
        batch = records[i:i+batch_size]
        try:
            supabase.table('house_ts').upsert(batch).execute()
            total_uploaded += len(batch)
            print(f"   Uploaded: {total_uploaded}/{len(records)}")
        except Exception as e:
            print(f"   ⚠️ Batch failed: {e}")
            print("\nMake sure you have updated the table in Supabase with this SQL:")
            print("""
            -- You may need to drop the old table first: DROP TABLE house_ts;
            create table house_ts (
              id bigint generated by default as identity primary key,
              zip_code text,
              date date,
              median_sale_price float,
              median_list_price float,
              median_ppsf float,
              homes_sold float,
              pending_sales float,
              new_listings float,
              inventory float,
              median_dom float,
              avg_sale_to_list float,
              sold_above_list float,
              created_at timestamp with time zone default timezone('utc'::text, now())
            );
            alter table house_ts add constraint house_ts_zip_date_key unique (zip_code, date);
            """)
            return

    print("✅ Upload successful!")

if __name__ == "__main__":
    upload_housets()
